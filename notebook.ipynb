{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d664fdc",
   "metadata": {},
   "source": [
    "# üöÄ Your Challenge: Boost Customer Retention for a Telco Company!\n",
    "Your mission is to help a telecom company predict whether a customer will leave (churn) in the next following months, \n",
    "and to develop strategies to keep them engaged and prevent revenue loss.\n",
    "\n",
    "By doing this, you‚Äôll enable the business to focus on retention programs for customers at risk, keeping them satisfied while reducing churn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5f435a5dc5e19",
   "metadata": {},
   "source": [
    "## üìä Data Overview\n",
    "You‚Äôre provided with **two datasets** that hold essential data about the customers:\n",
    "\n",
    "1.\t**customer_data**: Data about each customer\n",
    "2.\t**activity_data**: Monthly activity data from the year 2021\n",
    "### Customer Data\n",
    "- **customer_id (primary key)**: id of the customer (string - e.g. 100002)\n",
    "- **birth_date**: birth date of the customer (string - e.g. 1976-12-11 00:00:00)\n",
    "- **plan_type**: phone plan type (string - e.g.'pay-as-you-go', 'prepaid', 'postpaid')\n",
    "- **join_type**: the date the customer started using services (string - e.g. 2010-03-12 00:00:00)\n",
    "- **churn_in_3mos**: whether the customer left in the first 3 months of 2022\"? (boolean - 0 for active customers, 1 for departed customers)\n",
    "### Activity Data\n",
    "- **customer_id (primary foreign key)**: id of the customer (string - e.g. 100002)\n",
    "- **month (primary key)**: the billing period (string - e.g. 1/01/2021, format: dd/mm/yyyy, range: 12 months, from 1/01/2021 to 1/12/2021)\n",
    "- **data_usage**: how many GBs this customer used in a month (float - e.g. 21.23)\n",
    "- **phone_usage**: how many minutes this customer used in a month (float - e.g. 534.47)\n",
    "- **use_app**: whether the customer used the online app this month (boolean - 0 for customers who did not use the online app in the given month, 1 for customers who have used the online app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1eb160b2c95794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T11:06:09.727036Z",
     "start_time": "2024-10-28T11:06:09.725141Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Suggestion: to keep your notebook organized and clean, maintain a cell to manage your imports.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# import pandas as pd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Suggestion: to keep your notebook organized and clean, maintain a cell to manage your imports.\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876478121a0371cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T23:01:58.438799Z",
     "start_time": "2024-10-23T23:01:58.399780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "DATA_DIR = os.path.join(\"./\",\"data\")\n",
    "customer_data = pd.read_csv(os.path.join(DATA_DIR, \"customer_data.csv\"))\n",
    "activity_data = pd.read_csv(os.path.join(DATA_DIR, \"activity_data.csv\"))\n",
    "# customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791f06f35644bf1",
   "metadata": {},
   "source": [
    "### üîé Start with Some Exploration!\n",
    "\n",
    "Before jumping into building a predictive model, first **explore** the data to uncover any useful insights that can be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1961c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4463c0",
   "metadata": {},
   "source": [
    "### üß† Question 1: What‚Äôs the Average Tenure of Our Customers?\n",
    "\n",
    "The client wants to know how long their customers have been with them, as of 2022-01-01.\n",
    "\n",
    "**Task**: Calculate the **average tenure** (in years) of the customer base. Your function should return a number with **two decimal** places. \n",
    "\n",
    "This could help to identify loyal customers who might be at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e19bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T23:01:58.595439Z",
     "start_time": "2024-10-23T23:01:58.536686Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "def duration(join_date):\n",
    "    \n",
    "    join_date = datetime.strptime(join_date, '%Y-%m-%d')\n",
    "    today = datetime.strptime('2022-01-01', '%Y-%m-%d')\n",
    "    \n",
    "    # the tenure per customers\n",
    "    delta_year = (today - join_date).days / 365.25 # days transformed to years\n",
    "    print(delta_year)\n",
    "    return delta_year\n",
    "\n",
    "    \n",
    "# Calculate the average tenure of customers\n",
    "def avg_tenure(customer_data):\n",
    "    sum_duration = sum(duration(join_date) for join_date in customer_data['join_date'])\n",
    "    return round(sum_duration / len(customer_data), 2)\n",
    "    \n",
    "   \n",
    "average_tenure = avg_tenure(customer_data)\n",
    "print(\"Average Tenure:\", average_tenure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c5fad",
   "metadata": {},
   "source": [
    "### üì∂ Question 2: What‚Äôs the Average Data Usage by Plan Type?\n",
    "\n",
    "**Task**: Analyze how different types of customers are using data! Calculate the average monthly data usage for each plan type, and sort the results from low to high. \n",
    "\n",
    "This insight could be impactful for the marketing team to understand customer habits and plan targeted promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba152335",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-31T14:10:54.174379Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def avg_data_usage_by_type(customer_data, activity_data): \n",
    "    \n",
    "    activity_data_updated = activity_data.merge(customer_data, on = ['customer_id'], how = 'left')\n",
    "    \n",
    "    print(activity_data_updated)\n",
    "    \n",
    "    return activity_data_updated.groupby('plan_type')['data_usage'].mean()\n",
    "    \n",
    "    \n",
    "avg_data_usage_by_type(customer_data, activity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2d31b",
   "metadata": {},
   "source": [
    "### üíº Business Problem: Prevent Customer Churn\n",
    "The **marketing department** want to send personalized promotions to users who are **likely to churn**.\n",
    "\n",
    "Your task is to build a **classification model** that predicts the likelihood for a customer to churn in the next 3 months. The model will help focus **retention efforts** on the right customers, minimizing revenue loss.\n",
    "\n",
    "The target variable here is **churn_in_3mos** (1 = churned, 0 = active).\n",
    "\n",
    "### üîß Question 3a: Feature Engineering\n",
    "To make predictions, you‚Äôll first need to create new features using both customer and activity data. \n",
    "\n",
    "Here is a suggested list of relevant features that you should add to the aggregated table:\n",
    "- **tenure**: Customer tenure in years\n",
    "- **total_phone_usage**: Total phone minutes used in 2021\n",
    "- **app_usage_count**: Number of months the customer used the online app\n",
    "- **phone_usage_ratio**: Ratio of phone usage in the last 3 months vs the entire year\n",
    "\n",
    "Don‚Äôt forget to include the churn_in_3mos target variable!\n",
    "\n",
    "### üßë‚Äçüíª Question 3b: Can You Create Additional Features?\n",
    "Now that we have some basic features, can you **add 2 more features** that might help the model perform better? \n",
    "\n",
    "Think about what might influence customer behavior ‚Äî use your creativity!\n",
    "\n",
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Question 3c: Train a Churn Prediction Model\n",
    "It‚Äôs time to build our model! \n",
    "\n",
    "First, **split the data**: 80% for training and 20% for testing. \n",
    "\n",
    "Then, train your model to predict customer churn and calculate the AUC score on the test set.\n",
    "\n",
    "You should answer these three questions:\n",
    "\n",
    "1.\t**What is the AUC score?**\n",
    "2.\t**What is a good AUC score?**\n",
    "3.\t**What is one other evaluation metric** we could use for this problem? Why would it be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea6bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T23:02:02.547517Z",
     "start_time": "2024-10-23T23:01:58.821090Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57df19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T23:02:02.552901Z",
     "start_time": "2024-10-23T23:02:02.548758Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_set(activity_data, customer_data):\n",
    "    \"\"\"\n",
    "    Generate features\n",
    "    \"\"\"\n",
    "    # Clean data\n",
    "    activity_data_clean = activity_data.dropna()\n",
    "    customer_data_clean = customer_data.dropna()\n",
    "    \n",
    "    # phone_usage_ratio\n",
    "    phone_usage_last_3mos = activity_data_clean[activity_data_clean['month'].isin(['1/10/2021', '1/11/2021', '1/12/2021'])].groupby('customer_id')['phone_usage'].sum()\n",
    "    phone_usage_total = activity_data_clean.groupby('customer_id')['phone_usage'].sum()\n",
    "    phone_usage_ratio = phone_usage_last_3mos / phone_usage_total\n",
    "    \n",
    "    # total data usage\n",
    "    total_data_usage = activity_data_clean.groupby('customer_id')['data_usage'].sum()\n",
    "    \n",
    "    # app_usage_count\n",
    "    app_usage_count = activity_data_clean.groupby('customer_id')['use_app'].sum()\n",
    "    \n",
    "    # tenure\n",
    "    customer_data_clean['join_date'] = pd.to_datetime(customer_data_clean['join_date'])\n",
    "    customer_data_clean['tenure'] = (pd.to_datetime('2022-01-01') - customer_data_clean['join_date']).dt.days / 365\n",
    "    \n",
    "    # churn_in_3mos\n",
    "    churn_in_3mos = customer_data.groupby('customer_id')['churn_in_3mos'].sum()\n",
    "   \n",
    "    \n",
    "    # Combine features\n",
    "    features = pd.DataFrame({\n",
    "        'phone_usage_ratio': phone_usage_ratio,\n",
    "        'total_data_usage': total_data_usage,\n",
    "        'app_usage_count': app_usage_count,\n",
    "        'tenure': customer_data_clean.set_index('customer_id')['tenure'], # set_index to match the index('customer_id') of other features, becasuse when new dataFrame created, the index needs to be the same, otherwise, some data will be misaligned\n",
    "        'churn_in_3mos': churn_in_3mos\n",
    "    }).reset_index()\n",
    "    \n",
    "    return features\n",
    "\n",
    "dataset = feature_set(activity_data, customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647818c0",
   "metadata": {},
   "source": [
    "#### 3c. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da71ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T23:02:02.735712Z",
     "start_time": "2024-10-23T23:02:02.729976Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_test_model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Return the evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    return model\n",
    "    \n",
    "# Split the data into features and target\n",
    "X = dataset.drop(columns=['churn_in_3mos'])\n",
    "y = dataset['churn_in_3mos']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_test_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc58a8ca2613c22",
   "metadata": {},
   "source": [
    "### üéØ Final Challenge: Explaining the Model\n",
    "\n",
    "The last step is to make sure the client understands **why your model predicts** that certain customers will churn. \n",
    "\n",
    "Use an explainability framework to show which features are driving the predictions and ensure the solution is transparent and actionable!\n",
    "\n",
    "üí° Hint: `shap` is one of the most widely used library to generate such insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8dc208aa9e489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T14:13:00.714171Z",
     "start_time": "2024-10-31T14:13:00.466276Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_shap_plot_feature_importance(model):\n",
    "\n",
    "    # Create the SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "    # SHAP summary plot\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "# Run shap\n",
    "model = train_test_model(X_train, y_train, X_test, y_test)\n",
    "generate_shap_plot_feature_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c7c71560f931b",
   "metadata": {},
   "source": [
    "# ü§ñ GenAI Challenge: Customer Complaints Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec7bd6032f8817",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "**Customer Complaints Data**: You are provided with a dataset containing customer complaints from various channels. Each complaint includes a text description of the customer's issue and a category.\n",
    "\n",
    " - Complaint: The actual text of the customer complaint (string)\n",
    " - Customer ID: Unique identifier of the customer submitting the complaint\n",
    " - Category: The assigned category for each complaint (e.g., Billing Issue, Service Disruption)\n",
    " \n",
    "Your mission is to analyze each complaint, extract key information, and categorize the complaints into various actionable buckets to help the company take appropriate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fd339e3271ec7",
   "metadata": {},
   "source": [
    "# Challenge: Extracting Advanced Features from Customer Complaints\n",
    "In this challenge, you'll process the complaints to extract key insights, including:\n",
    "\n",
    "- **Key Issues**: Extracting the most relevant keywords from the complaint that describe the main issue.\n",
    "- **Sentiment Analysis**: Determining the sentiment expressed by the customer (neutral, negative, extremely negative).\n",
    "- **Severity Rating**: Rating the severity of the complaint on a scale from 1 to 10, based on the impact and seriousness of the issue.\n",
    "\n",
    "And try to categorize each complaints into actionable buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce4f9f4b47fd64",
   "metadata": {},
   "source": [
    "# Improving with LLMs\n",
    "The only method we will use to improve the LLM's performance in this exercise will be through prompting. You will be tasked with crafting effective prompts to extract the required features from the complaints, rather than fine-tuning the model or making architectural changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8320d7394b9921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T11:05:28.970951Z",
     "start_time": "2024-10-28T11:05:27.580580Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, Runnable\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc8d66d90544f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:43:50.042254Z",
     "start_time": "2024-10-24T09:43:48.821017Z"
    }
   },
   "outputs": [],
   "source": [
    "login(token='', add_to_git_credential=True)\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9570fde87f8863f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:43:50.209198Z",
     "start_time": "2024-10-24T09:43:50.043391Z"
    }
   },
   "outputs": [],
   "source": [
    "df_complaints = pd.read_excel('data/complaints.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6e051789b2316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:43:50.736856Z",
     "start_time": "2024-10-24T09:43:50.724547Z"
    }
   },
   "outputs": [],
   "source": [
    "# instanciate llm\n",
    "llm_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "hf_llm = HuggingFaceEndpoint(\n",
    "    repo_id=llm_model,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token='',\n",
    ")\n",
    "\n",
    "print(hf_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546f95b8607985f",
   "metadata": {},
   "source": [
    "# Question 1: \n",
    "- build the chain for the llm \n",
    "- create the prompt for the llm\n",
    "- extract the key insights listed above from the complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda72f2f3e4d52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:43:52.724409Z",
     "start_time": "2024-10-24T09:43:52.718934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class to extract feature from complaints\n",
    "class ExtractKeywords(Runnable):\n",
    "    def __init__(self):\n",
    "        return \n",
    "    \n",
    "         \n",
    "\n",
    "\n",
    "# Define the trasformation chain that are applied to the complaint\n",
    "# - the chain start with reading the complaint \n",
    "# - the class to extract the feature is applied\n",
    "# - the result is given to the LLM\n",
    "# - A parser is applied\n",
    "\n",
    " chain = (\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ebcd432de7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process all the df row by row\n",
    "def process_df_with_chain(df):\n",
    "\n",
    "    return df\n",
    "\n",
    "df_complaints_processed = process_df_with_chain(df_complaints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939d6e079c3684",
   "metadata": {},
   "source": [
    "# Question 2: Parallel Processing and Optimization:\n",
    "In large datasets, processing customer complaints sequentially can be time-consuming. How would you optimize this task using parallel processing techniques? Can you identify potential challenges when scaling this approach, especially with regards to memory management and error handling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b05b79f9fe369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "\n",
    "# Function called from the thread to invoke the chain\n",
    "def process_complaint_with_chain(complaint):\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing complaint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to parallelize the dataframe elaboration using ThreadPoolExecutor\n",
    "def process_df_with_chain_parallel(df):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "df_complaints_processed = process_df_with_chain_parallel(df_complaints)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fca84c3352db2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a644b99cddb4e4",
   "metadata": {},
   "source": [
    "# Question 3: Cluster the complaints in differents buckets and suggest actionable insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006fafba6f25dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d9eb05974ad61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
